{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL Quiz\n",
    "\n",
    "This quiz uses the same dataset and questions from the Spark Data Frames Programming Quiz. For this quiz, however, use Spark SQL instead of Spark Data Frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql import types\n",
    "# TODOS: \n",
    "# 1) import any other libraries you might need\n",
    "# 2) instantiate a Spark session \n",
    "# 3) read in the data set located at the path \"data/sparkify_log_small.json\"\n",
    "# 4) create a view to use with your SQL queries\n",
    "# 5) write code to answer the quiz questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"My Spark SQL Quiz\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_data = spark.read.json(\"data/sparkify_log_small.json\")\n",
    "log_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Which page did user id \"\"(empty string) NOT visit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data.createOrReplaceTempView(\"log_data_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|            page|\n",
      "+----------------+\n",
      "|Submit Downgrade|\n",
      "|       Downgrade|\n",
      "|          Logout|\n",
      "|   Save Settings|\n",
      "|        Settings|\n",
      "|        NextSong|\n",
      "|         Upgrade|\n",
      "|           Error|\n",
      "|  Submit Upgrade|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT DISTINCT page\n",
    "    FROM log_data_table\n",
    "    WHERE page NOT IN (SELECT DISTINCT page\n",
    "                        FROM log_data_table\n",
    "                        WHERE userId == '')\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) HashAggregate(keys=[page#16], functions=[])\n",
      "+- Exchange hashpartitioning(page#16, 200)\n",
      "   +- *(4) HashAggregate(keys=[page#16], functions=[])\n",
      "      +- *(4) Project [page#16]\n",
      "         +- BroadcastNestedLoopJoin BuildRight, LeftAnti, ((page#16 = page#16#77) || isnull((page#16 = page#16#77)))\n",
      "            :- *(1) FileScan json [page#16] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/home/workspace/data/sparkify_log_small.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<page:string>\n",
      "            +- BroadcastExchange IdentityBroadcastMode\n",
      "               +- *(3) HashAggregate(keys=[page#16], functions=[])\n",
      "                  +- Exchange hashpartitioning(page#16, 200)\n",
      "                     +- *(2) HashAggregate(keys=[page#16], functions=[])\n",
      "                        +- *(2) Project [page#16]\n",
      "                           +- *(2) Filter (isnotnull(userId#23) && (userId#23 = ))\n",
      "                              +- *(2) FileScan json [page#16,userId#23] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/home/workspace/data/sparkify_log_small.json], PartitionFilters: [], PushedFilters: [IsNotNull(userId), EqualTo(userId,)], ReadSchema: struct<page:string,userId:string>\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT DISTINCT page\n",
    "    FROM log_data_table\n",
    "    WHERE page NOT IN (SELECT DISTINCT page\n",
    "                        FROM log_data_table\n",
    "                        WHERE userId == '')\n",
    "\"\"\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - Reflect\n",
    "\n",
    "Why might you prefer to use SQL over data frames? Why might you prefer data frames over SQL?\n",
    "Team may already be familiar or even skilled in SQL than Data Frames / Pandas. Some operations are just easier to do with SQL.. especially operations requiring JOINing two data sets\n",
    "Data Frames - Offer more flexibility and allows for imperative programming. Shareable and increase in adaptation by Data Science community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "How many female users do we have in the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT userId)|\n",
      "+----------------------+\n",
      "|                   462|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT userId)\n",
    "    FROM log_data_table\n",
    "    WHERE gender == 'F'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "How many songs were played from the most played artist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|  artist|num_songs|\n",
      "+--------+---------+\n",
      "|Coldplay|       83|\n",
      "+--------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT artist, count(*) as num_songs\n",
    "    FROM log_data_table\n",
    "    WHERE page == 'NextSong'\n",
    "    GROUP BY artist\n",
    "    ORDER BY num_songs DESC\n",
    "\"\"\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 (challenge)\n",
    "\n",
    "How many songs do users listen to on average between visiting our home page? Please round your answer to the closest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_by_user_phase = spark.sql(\"\"\"\n",
    "    SELECT userId, ts, song_play, \n",
    "    SUM(homepage_visit) OVER (PARTITION BY userId ORDER BY int(ts) DESC RANGE UNBOUNDED PRECEDING) as phase\n",
    "    FROM (\n",
    "        SELECT userId, ts,\n",
    "        CASE WHEN page == 'Home' THEN 1 ELSE 0 END as homepage_visit,\n",
    "        CASE WHEN page == 'NextSong' THEN 1 ELSE 0 END as song_play\n",
    "        FROM log_data_table\n",
    "        WHERE page == 'NextSong' OR page == 'Home'\n",
    "        ) sub\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|             avg1|             avg2|\n",
      "+-----------------+-----------------+\n",
      "|4.432819968135953|6.898347107438017|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_by_user_phase.createOrReplaceTempView(\"songs_by_user_phase_table\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT AVG(num_songs) AS avg1, AVG(CASE WHEN num_songs == 0 THEN NULL ELSE num_songs END) AS avg2\n",
    "    FROM (\n",
    "        SELECT userId, phase, SUM(song_play) AS num_songs\n",
    "        FROM songs_by_user_phase_table\n",
    "        GROUP BY userId, phase\n",
    "        ) sub\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
